{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcRRp2XRLOdUBMMGrOzjwZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhamu2908/CS6370-NLP/blob/main/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QSjIG3g4fHEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "usOLWbMyhXpJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Three Documents Given***"
      ],
      "metadata": {
        "id": "3wnBGSNff_rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = \"Herbivores are typically plant eaters and not meat eaters\"\n",
        "d2 = \"Carnivores are typically meat eaters and not plant eaters\"\n",
        "d3 = \"Deers eat grass and leaves\""
      ],
      "metadata": {
        "id": "U505qWV8fHBr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing each document using the stop words** **[are, and, not]**"
      ],
      "metadata": {
        "id": "M5UATBfSgZ36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = ['are', 'and', 'not']\n",
        "\n",
        "def preprocess(doc):\n",
        "    tokens = word_tokenize(doc)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.lower() not in stop_words]\n",
        "    return processed_tokens # Added return statement to return processed tokens\n",
        "\n",
        "PreProcessed_d1 = preprocess(d1)\n",
        "PreProcessed_d2 = preprocess(d2)\n",
        "PreProcessed_d3 = preprocess(d3)"
      ],
      "metadata": {
        "id": "xEk8r31KfG-s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('d1 = ', PreProcessed_d1)\n",
        "print('d2 = ',PreProcessed_d2)\n",
        "print('d3 = ',PreProcessed_d3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp1vP_wnfG8H",
        "outputId": "6a51bbe7-a050-444d-c37f-ea8845862adf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d1 =  ['Herbivores', 'typically', 'plant', 'eater', 'meat', 'eater']\n",
            "d2 =  ['Carnivores', 'typically', 'meat', 'eater', 'plant', 'eater']\n",
            "d3 =  ['Deers', 'eat', 'grass', 'leaf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part - 1**\n",
        "\n",
        "***2 . TF-IDF term-document matrix for the corpus {d1, d2, d3}.***"
      ],
      "metadata": {
        "id": "2czESeB_DyJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# Define stop words and preprocessing function\n",
        "stop_words = ['are', 'and', 'not']\n",
        "def preprocess(doc):\n",
        "    tokens = word_tokenize(doc)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    processed_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]\n",
        "    return processed_tokens\n",
        "\n",
        "# Input documents\n",
        "d1 = \"Herbivores are typically plant eaters and not meat eaters\"\n",
        "d2 = \"Carnivores are typically meat eaters and not plant eaters\"\n",
        "d3 = \"Deers eat grass and leaves\"\n",
        "\n",
        "# Preprocess the documents\n",
        "docs = [preprocess(d1), preprocess(d2), preprocess(d3)]\n",
        "\n",
        "# Build the vocabulary\n",
        "vocab = set(word for doc in docs for word in doc)\n",
        "\n",
        "# Inverted index\n",
        "inverted_index = {word: [int(word in doc) for doc in docs] for word in vocab}\n",
        "\n",
        "# Document frequency and IDF (with base 2)\n",
        "df = {word: sum(inverted_index[word]) for word in vocab}\n",
        "N = len(docs)\n",
        "idf = {word: round(math.log(N / df[word], 2), 4) for word in vocab}\n",
        "\n",
        "# Compute TF and TF-IDF\n",
        "tf_list = []\n",
        "tfidf_matrix = []\n",
        "\n",
        "for doc in docs:\n",
        "    tf = Counter(doc)  # raw frequency (count)\n",
        "    tf_list.append(tf)\n",
        "    tfidf = {word: round(tf[word] * idf[word], 4) if word in tf else 0 for word in vocab}\n",
        "    tfidf_matrix.append(tfidf)\n",
        "\n",
        "# Display output\n",
        "print(f\"{'Word':<12} {'Inverted Index':<20} {'TF_d1':<6} {'TF_d2':<6} {'TF_d3':<6} {'IDF':<8} {'TFIDF_d1':<12} {'TFIDF_d2':<12} {'TFIDF_d3':<12}\")\n",
        "print(\"-\" * 100)\n",
        "for word in vocab:\n",
        "    inv = inverted_index[word]\n",
        "    tf1 = tf_list[0].get(word, 0)\n",
        "    tf2 = tf_list[1].get(word, 0)\n",
        "    tf3 = tf_list[2].get(word, 0)\n",
        "    idf_val = idf[word]\n",
        "    tfidf1 = tfidf_matrix[0][word]\n",
        "    tfidf2 = tfidf_matrix[1][word]\n",
        "    tfidf3 = tfidf_matrix[2][word]\n",
        "    print(f\"{word:<12} {str(inv):<20} {tf1:<6} {tf2:<6} {tf3:<6} {idf_val:<8.4f} {tfidf1:<12.4f} {tfidf2:<12.4f} {tfidf3:<12.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-GW5XZALX4-",
        "outputId": "769487a9-fda9-4bc2-e576-5b13463c9a3d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word         Inverted Index       TF_d1  TF_d2  TF_d3  IDF      TFIDF_d1     TFIDF_d2     TFIDF_d3    \n",
            "----------------------------------------------------------------------------------------------------\n",
            "herbivore    [1, 0, 0]            1      0      0      1.5850   1.5850       0.0000       0.0000      \n",
            "eater        [1, 1, 0]            2      2      0      0.5850   1.1700       1.1700       0.0000      \n",
            "carnivore    [0, 1, 0]            0      1      0      1.5850   0.0000       1.5850       0.0000      \n",
            "typically    [1, 1, 0]            1      1      0      0.5850   0.5850       0.5850       0.0000      \n",
            "deer         [0, 0, 1]            0      0      1      1.5850   0.0000       0.0000       1.5850      \n",
            "eat          [0, 0, 1]            0      0      1      1.5850   0.0000       0.0000       1.5850      \n",
            "grass        [0, 0, 1]            0      0      1      1.5850   0.0000       0.0000       1.5850      \n",
            "meat         [1, 1, 0]            1      1      0      0.5850   0.5850       0.5850       0.0000      \n",
            "plant        [1, 1, 0]            1      1      0      0.5850   0.5850       0.5850       0.0000      \n",
            "leaf         [0, 0, 1]            0      0      1      1.5850   0.0000       0.0000       1.5850      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3NKOzMumKki1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gjpN3xjzLXo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "COV1dHg4LXre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Wuco7ClLXt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ra-oinxRLXwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XkYaEdhhLXzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dd-spUHoLX1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qBJP_tMZLYYW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}